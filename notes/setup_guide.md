# üõ†Ô∏è Setup Guide for AWS Glue De-Identifier Project

This document guides you through the setup and deployment of the **AWS Glue De-Identifier** project.

---

## üì¶ Prerequisites

Before you begin:

- ‚úÖ AWS account
- ‚úÖ IAM permissions for S3, Glue, and Glue Studio
- ‚úÖ Python 3.8+ installed locally
- ‚úÖ Streamlit installed
- ‚úÖ Basic understanding of S3, IAM, and Glue jobs

---

## 1Ô∏è‚É£ S3 Bucket Setup

Create two buckets in your AWS account:

| Bucket Name          | Purpose                  |
|----------------------|---------------------------|
| `healthcare-upload`  | Stores raw uploaded CSVs  |
| `healthcare-output`  | Stores de-identified data |

> üìçMake sure bucket names are globally unique (e.g., `yourname-healthcare-upload`)

---

## 2Ô∏è‚É£ Create the AWS Glue Job

### A. Go to AWS Glue Studio ‚Üí **Jobs**
1. Click **Create job**
2. Use **Visual with a source and target**
3. Source ‚Üí S3 (CSV) ‚Üí `s3://healthcare-upload/`
4. Target ‚Üí S3 ‚Üí `s3://healthcare-output/`

### B. Add Custom Transform
1. Add a **Transform node**
2. Choose **Custom transform**
3. Paste the script from `glue_jobs/deidentify_job_script.py`

### C. Job Parameters
Add two job arguments so the frontend can dynamically pass them:
```text
--input_path
--output_path

```

Then in the source node:

Change data location to: Dynamic path: ${input_path}

And in the target node:

Set output location to: ${output_path}

‚úÖ This makes your Glue job reusable for each file upload

---

## 3Ô∏è‚É£ Configure IAM Role
### Create or use an IAM role for Glue with:

* AmazonS3FullAccess

* AWSGlueServiceRole

Also grant the same or limited permissions to the credentials used in your Streamlit app:

* glue:StartJobRun, glue:GetJobRun

* s3:PutObject, s3:GetObject

---

## 4Ô∏è‚É£ Launch the Streamlit Frontend

### A. Navigate to the frontend folder

``
cd streamlit_app
``
### B. Install dependencies
``
pip install -r requirements.txt
``
### C. Run the web app
``
streamlit run app.py
``
### D. Upload Sample File
Upload a file like sample_input.csv, then wait for Glue job completion and download the cleaned file.

---

## 5Ô∏è‚É£ Optional: AWS Athena Setup (for Querying Output)
If you want to query the results:

1. Go to AWS Glue ‚Üí Crawlers

2. Create a crawler for s3://healthcare-output/

3. Create a database (e.g., healthcare_db)

4. Run the crawler

5. Go to Athena ‚Üí Select database ‚Üí Run:

``
SELECT * FROM patient_deid LIMIT 10;
``
---
## ‚úÖ Final Checklist
```
| Task                           | Status |
| ------------------------------ | ------ |
| Create 2 S3 buckets            | ‚úÖ      |
| Build and test Glue job        | ‚úÖ      |
| Add dynamic input/output paths | ‚úÖ      |
| Set up Streamlit frontend      | ‚úÖ      |
| Trigger job and verify results | ‚úÖ      |
| (Optional) Add Athena access   | ‚úÖ      |
```
---
## ü§ù Support
For help, contact:
Ahmed Elbashier
[LinkedIn](https://www.linkedin.com/in/ahmed-elbashier) | [GitHub](https://github.com/AhmedElbashier)



